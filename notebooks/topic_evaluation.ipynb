{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling Evaluation\n",
    "\n",
    "This notebook evaluates **all topic versions** in the database using multiple quantitative metrics:\n",
    "\n",
    "## Metrics Computed\n",
    "\n",
    "1. **Coherence Metrics** (C_v, NPMI, U_Mass) - measures how well topics capture patterns in your news articles\n",
    "2. **Topic Diversity** - Vocabulary overlap, unique words ratio\n",
    "3. **Inter-Topic Similarity** - Embedding-based and keyword-based\n",
    "4. **Assignment Quality** - Confidence scores, coverage, outlier ratio\n",
    "5. **Silhouette Score** - Clustering quality in embedding space\n",
    "6. **Keyword Informativeness** - TF-IDF distinctiveness\n",
    "7. **Composite Score** - Weighted combination for ranking\n",
    "8. **Per-Topic Analysis** - Individual topic quality within versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs\n",
    "\n",
    "- **CSVs**: Consolidated metrics, per-topic coherence, summary statistics\n",
    "- **Visualizations**: Interactive Plotly charts\n",
    "- **Recommendations**: Best version identification and rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports & Configuration\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from bertopic import BERTopic\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "import yaml\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')\n",
    "from src.db import get_db\n",
    "from src.versions import list_versions\n",
    "from dashboard.data.loaders import load_bertopic_model\n",
    "\n",
    "print(\"All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Configuration & List Versions\n",
    "# Load database config\n",
    "with open('../config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# List all topic versions (evaluates ALL versions in database)\n",
    "versions_list = list_versions(analysis_type='topics')\n",
    "versions_df = pd.DataFrame(versions_list)\n",
    "print(f\"Found {len(versions_df)} topic versions for evaluation\")\n",
    "display(versions_df[['id', 'name', 'description', 'created_at']])\n",
    "\n",
    "# Evaluate all versions\n",
    "version_ids = versions_df['id'].tolist()\n",
    "print(f\"\\nEvaluating {len(version_ids)} versions: {versions_df['name'].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Topic Coherence Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Helper Functions for Data Extraction\n",
    "def load_version_data(version_id):\n",
    "    \"\"\"Load all data needed for evaluation from database\"\"\"\n",
    "    with get_db() as db:\n",
    "        conn = db._conn\n",
    "        schema = db.config['schema']\n",
    "        \n",
    "        # Load topics (excluding outliers)\n",
    "        # NOTE: We need both 'id' (PK, referenced by article_analysis.primary_topic_id)\n",
    "        # and 'topic_id' (BERTopic's topic number)\n",
    "        topics_df = pd.read_sql(f\"\"\"\n",
    "            SELECT id as topic_pk, topic_id, name, description, keywords, article_count\n",
    "            FROM {schema}.topics\n",
    "            WHERE result_version_id = %s AND topic_id != -1\n",
    "            ORDER BY article_count DESC\n",
    "        \"\"\", conn, params=(version_id,))\n",
    "        \n",
    "        # Load article-topic assignments\n",
    "        # NOTE: primary_topic_id references topics.id (the PK), not topics.topic_id\n",
    "        assignments_df = pd.read_sql(f\"\"\"\n",
    "            SELECT aa.article_id, aa.primary_topic_id, aa.topic_confidence,\n",
    "                   na.title, na.content\n",
    "            FROM {schema}.article_analysis aa\n",
    "            JOIN {schema}.news_articles na ON aa.article_id = na.id\n",
    "            WHERE aa.result_version_id = %s\n",
    "        \"\"\", conn, params=(version_id,))\n",
    "        \n",
    "        # Load embeddings\n",
    "        embeddings_df = pd.read_sql(f\"\"\"\n",
    "            SELECT e.article_id, e.embedding\n",
    "            FROM {schema}.embeddings e\n",
    "            WHERE e.result_version_id = %s\n",
    "        \"\"\", conn, params=(version_id,))\n",
    "        \n",
    "        # Convert embeddings from string to numpy array if needed\n",
    "        if len(embeddings_df) > 0 and isinstance(embeddings_df['embedding'].iloc[0], str):\n",
    "            embeddings_df['embedding'] = embeddings_df['embedding'].apply(\n",
    "                lambda x: np.fromstring(x.strip('[]'), sep=',')\n",
    "            )\n",
    "        \n",
    "        # Load BERTopic model\n",
    "        try:\n",
    "            model = load_bertopic_model(version_id)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load BERTopic model for version {version_id}: {e}\")\n",
    "            model = None\n",
    "    \n",
    "    return {\n",
    "        'topics': topics_df,\n",
    "        'assignments': assignments_df,\n",
    "        'embeddings': embeddings_df,\n",
    "        'model': model\n",
    "    }\n",
    "\n",
    "def prepare_corpus_for_coherence(assignments_df, bertopic_model):\n",
    "    \"\"\"Prepare corpus and dictionary for Gensim coherence calculation\n",
    "    \n",
    "    Uses BERTopic's vectorizer to ensure coherence is calculated on the same\n",
    "    vocabulary space that BERTopic used for topic modeling. This provides a\n",
    "    more accurate assessment of topic quality.\n",
    "    \"\"\"\n",
    "\n",
    "    texts_raw = [\n",
    "        str(row['title']) + ' ' + str(row['content'])\n",
    "        for _, row in assignments_df.iterrows()\n",
    "    ]\n",
    "    \n",
    "    # Use BERTopic's vectorizer to tokenize (same vocabulary/preprocessing as BERTopic)\n",
    "    vectorizer = bertopic_model.vectorizer_model\n",
    "    texts = [vectorizer.build_analyzer()(doc) for doc in texts_raw]\n",
    "    \n",
    "    dictionary = Dictionary(texts)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    \n",
    "    return texts, dictionary, corpus\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Data Loading Loop\n",
    "# Load data for all versions\n",
    "version_data = {}\n",
    "for version_id in version_ids:\n",
    "    print(f\"Loading data for version {version_id}...\")\n",
    "    version_data[version_id] = load_version_data(version_id)\n",
    "    print(f\"  Topics: {len(version_data[version_id]['topics'])}\")\n",
    "    print(f\"  Assignments: {len(version_data[version_id]['assignments'])}\")\n",
    "    print(f\"  Embeddings: {len(version_data[version_id]['embeddings'])}\")\n",
    "\n",
    "print(f\"\\nData loaded for {len(version_data)} versions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Compute Coherence Scores\n",
    "def compute_coherence_scores(topics_df, texts, dictionary, corpus):\n",
    "    \"\"\"Compute coherence scores using the same corpus that generated topics\n",
    "    \n",
    "    Args:\n",
    "        topics_df: DataFrame with topic keywords\n",
    "        texts: Tokenized article texts\n",
    "        dictionary: Gensim dictionary\n",
    "        corpus: Bag-of-words corpus\n",
    "    \n",
    "    Returns:\n",
    "        dict with coherence scores\n",
    "    \"\"\"\n",
    "    topics_keywords = []\n",
    "    for _, row in topics_df.iterrows():\n",
    "        keywords = row['keywords']\n",
    "        topics_keywords.append(keywords)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(\"  Computing coherence scores...\")\n",
    "    \n",
    "    print(\"    - C_v...\")\n",
    "    coherence_cv = CoherenceModel(\n",
    "        topics=topics_keywords,\n",
    "        texts=texts,\n",
    "        dictionary=dictionary,\n",
    "        coherence='c_v'\n",
    "    )\n",
    "    results['c_v'] = coherence_cv.get_coherence()\n",
    "    results['c_v_per_topic'] = coherence_cv.get_coherence_per_topic()\n",
    "    \n",
    "    print(\"    - NPMI...\")\n",
    "    coherence_npmi = CoherenceModel(\n",
    "        topics=topics_keywords,\n",
    "        texts=texts,\n",
    "        dictionary=dictionary,\n",
    "        coherence='c_npmi'\n",
    "    )\n",
    "    results['c_npmi'] = coherence_npmi.get_coherence()\n",
    "    results['c_npmi_per_topic'] = coherence_npmi.get_coherence_per_topic()\n",
    "    \n",
    "    print(\"    - U_Mass...\")\n",
    "    coherence_umass = CoherenceModel(\n",
    "        topics=topics_keywords,\n",
    "        corpus=corpus,\n",
    "        dictionary=dictionary,\n",
    "        coherence='u_mass'\n",
    "    )\n",
    "    results['u_mass'] = coherence_umass.get_coherence()\n",
    "    results['u_mass_per_topic'] = coherence_umass.get_coherence_per_topic()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute coherence for all versions\n",
    "coherence_results = {}\n",
    "for version_id in version_ids:\n",
    "    print(f\"\\nComputing coherence for version {version_id}...\")\n",
    "    data = version_data[version_id]\n",
    "    \n",
    "    # Check if model is available\n",
    "    if data['model'] is None:\n",
    "        print(f\"  Skipping: BERTopic model not available\")\n",
    "        continue\n",
    "    \n",
    "    # Use BERTopic's vectorizer for corpus preparation\n",
    "    texts, dictionary, corpus = prepare_corpus_for_coherence(\n",
    "        data['assignments'], data['model']\n",
    "    )\n",
    "    \n",
    "    # Compute coherence scores\n",
    "    coherence_results[version_id] = compute_coherence_scores(\n",
    "        data['topics'], texts, dictionary, corpus\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"  Results:\")\n",
    "    print(f\"    C_v: {coherence_results[version_id]['c_v']:.3f}\")\n",
    "    print(f\"    NPMI: {coherence_results[version_id]['c_npmi']:.3f}\")\n",
    "    print(f\"    U_Mass: {coherence_results[version_id]['u_mass']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Coherence Results DataFrame\n",
    "coherence_df = pd.DataFrame([\n",
    "    {\n",
    "        'version_id': vid,\n",
    "        'version_name': versions_df[versions_df['id']==vid]['name'].values[0],\n",
    "        'c_v': results['c_v'],\n",
    "        'c_npmi': results['c_npmi'],\n",
    "        'u_mass': results['u_mass']\n",
    "    }\n",
    "    for vid, results in coherence_results.items()\n",
    "])\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COHERENCE SCORES BY VERSION\")\n",
    "print(\"=\" * 80)\n",
    "display(coherence_df[['version_name', 'c_v', 'c_npmi', 'u_mass']].sort_values('c_v', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Coherence Visualization\n",
    "# Bar chart comparing coherence scores\n",
    "fig = go.Figure()\n",
    "for metric, label in [('c_v', 'C_v'), ('c_npmi', 'NPMI'), ('u_mass', 'U_Mass')]:\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=label,\n",
    "        x=coherence_df['version_name'],\n",
    "        y=coherence_df[metric]\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Coherence Scores Across Versions',\n",
    "    xaxis_title='Version',\n",
    "    yaxis_title='Coherence Score',\n",
    "    barmode='group',\n",
    "    height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Topic Diversity Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Compute Topic Diversity\n",
    "def compute_topic_diversity(topics_df, top_n=10):\n",
    "    \"\"\"Compute vocabulary overlap and uniqueness metrics\"\"\"\n",
    "    # Extract top-N keywords per topic\n",
    "    topic_words = [set(row['keywords'][:top_n]) for _, row in topics_df.iterrows()]\n",
    "    \n",
    "    # Pairwise Jaccard similarity\n",
    "    jaccard_scores = []\n",
    "    for i in range(len(topic_words)):\n",
    "        for j in range(i+1, len(topic_words)):\n",
    "            intersection = len(topic_words[i] & topic_words[j])\n",
    "            union = len(topic_words[i] | topic_words[j])\n",
    "            jaccard = intersection / union if union > 0 else 0\n",
    "            jaccard_scores.append(jaccard)\n",
    "    \n",
    "    avg_overlap = np.mean(jaccard_scores) if jaccard_scores else 0\n",
    "    \n",
    "    # Unique words ratio\n",
    "    all_words = [word for words in topic_words for word in words]\n",
    "    unique_words = len(set(all_words))\n",
    "    total_words = len(all_words)\n",
    "    unique_ratio = unique_words / total_words if total_words > 0 else 0\n",
    "    \n",
    "    # Top-word diversity (words appearing in only one topic)\n",
    "    word_counts = Counter(all_words)\n",
    "    unique_count = sum(1 for count in word_counts.values() if count == 1)\n",
    "    top_word_diversity = unique_count / total_words if total_words > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'avg_jaccard_overlap': avg_overlap,\n",
    "        'unique_words_ratio': unique_ratio,\n",
    "        'top_word_diversity': top_word_diversity,\n",
    "        'num_topics': len(topic_words),\n",
    "        'total_unique_words': unique_words\n",
    "    }\n",
    "\n",
    "# Compute for all versions\n",
    "diversity_results = {}\n",
    "for version_id in version_ids:\n",
    "    data = version_data[version_id]\n",
    "    diversity_results[version_id] = compute_topic_diversity(data['topics'])\n",
    "\n",
    "print(\"Diversity computation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Diversity Results DataFrame\n",
    "diversity_df = pd.DataFrame([\n",
    "    {\n",
    "        'version_id': vid,\n",
    "        'version_name': versions_df[versions_df['id']==vid]['name'].values[0],\n",
    "        **results\n",
    "    }\n",
    "    for vid, results in diversity_results.items()\n",
    "])\n",
    "\n",
    "print(\"Topic Diversity Metrics:\")\n",
    "display(diversity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Diversity Visualization\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Avg Jaccard Overlap (lower is better)',\n",
    "    x=diversity_df['version_name'],\n",
    "    y=diversity_df['avg_jaccard_overlap'],\n",
    "    marker_color='indianred'\n",
    "))\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Unique Words Ratio (higher is better)',\n",
    "    x=diversity_df['version_name'],\n",
    "    y=diversity_df['unique_words_ratio'],\n",
    "    marker_color='lightseagreen'\n",
    "))\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Top-Word Diversity (higher is better)',\n",
    "    x=diversity_df['version_name'],\n",
    "    y=diversity_df['top_word_diversity'],\n",
    "    marker_color='lightsalmon'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Topic Diversity Metrics',\n",
    "    xaxis_title='Version',\n",
    "    yaxis_title='Score',\n",
    "    barmode='group',\n",
    "    height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Inter-Topic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Compute Inter-Topic Similarity\n",
    "def compute_inter_topic_similarity(topics_df, embeddings_df, assignments_df):\n",
    "    \"\"\"Compute topic similarities using embeddings\"\"\"\n",
    "    # Create topic embeddings (centroid of article embeddings)\n",
    "    topic_embeddings = {}\n",
    "    \n",
    "    for _, topic in topics_df.iterrows():\n",
    "        # Use topic_pk (the primary key) to match with assignments.primary_topic_id\n",
    "        topic_pk = topic['topic_pk']\n",
    "        topic_id = topic['topic_id']  # BERTopic's topic number (for display/results)\n",
    "        \n",
    "        # Get articles assigned to this topic\n",
    "        # NOTE: assignments.primary_topic_id references topics.id (topic_pk), not topics.topic_id\n",
    "        article_ids = assignments_df[\n",
    "            assignments_df['primary_topic_id'] == topic_pk\n",
    "        ]['article_id'].tolist()\n",
    "\n",
    "        # Get embeddings for these articles\n",
    "        topic_vecs = embeddings_df[\n",
    "            embeddings_df['article_id'].isin(article_ids)\n",
    "        ]['embedding'].tolist()\n",
    "\n",
    "        if topic_vecs:\n",
    "            # Compute centroid - use topic_id as the key for results\n",
    "            topic_embeddings[topic_id] = np.mean(topic_vecs, axis=0)\n",
    "    \n",
    "    # Compute pairwise cosine similarities\n",
    "    topic_ids = list(topic_embeddings.keys())\n",
    "    n_topics = len(topic_ids)\n",
    "    \n",
    "    print(f\"  Found embeddings for {n_topics} topics out of {len(topics_df)}\")\n",
    "    \n",
    "    # Handle edge case: fewer than 2 topics\n",
    "    if n_topics < 2:\n",
    "        print(\"  Not enough topics with embeddings to compute similarities.\")\n",
    "        return {\n",
    "            'mean_similarity': np.nan,\n",
    "            'median_similarity': np.nan,\n",
    "            'std_similarity': np.nan,\n",
    "            'max_similarity': np.nan,\n",
    "            'min_similarity': np.nan,\n",
    "            'similarity_matrix': np.array([]),\n",
    "            'topic_ids': topic_ids\n",
    "        }\n",
    "    \n",
    "    similarity_matrix = np.zeros((n_topics, n_topics))\n",
    "    \n",
    "    for i, tid1 in enumerate(topic_ids):\n",
    "        for j, tid2 in enumerate(topic_ids):\n",
    "            if i != j:\n",
    "                sim = 1 - cosine(topic_embeddings[tid1], topic_embeddings[tid2])\n",
    "                similarity_matrix[i, j] = sim\n",
    "    \n",
    "    # Extract upper triangle (exclude diagonal)\n",
    "    upper_tri = similarity_matrix[np.triu_indices(n_topics, k=1)]\n",
    "    \n",
    "    return {\n",
    "        'mean_similarity': np.mean(upper_tri),\n",
    "        'median_similarity': np.median(upper_tri),\n",
    "        'std_similarity': np.std(upper_tri),\n",
    "        'max_similarity': np.max(upper_tri),\n",
    "        'min_similarity': np.min(upper_tri),\n",
    "        'similarity_matrix': similarity_matrix,\n",
    "        'topic_ids': topic_ids\n",
    "    }\n",
    "\n",
    "# Compute for all versions\n",
    "similarity_results = {}\n",
    "for version_id in version_ids:\n",
    "    print(f\"Computing inter-topic similarity for {version_id}...\")\n",
    "    data = version_data[version_id]\n",
    "    similarity_results[version_id] = compute_inter_topic_similarity(\n",
    "        data['topics'], data['embeddings'], data['assignments']\n",
    "    )\n",
    "\n",
    "print(\"\\nSimilarity computation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Similarity Results DataFrame\n",
    "similarity_df = pd.DataFrame([\n",
    "    {\n",
    "        'version_id': vid,\n",
    "        'version_name': versions_df[versions_df['id']==vid]['name'].values[0],\n",
    "        'mean_similarity': results['mean_similarity'],\n",
    "        'median_similarity': results['median_similarity'],\n",
    "        'std_similarity': results['std_similarity'],\n",
    "        'max_similarity': results['max_similarity'],\n",
    "        'min_similarity': results['min_similarity']\n",
    "    }\n",
    "    for vid, results in similarity_results.items()\n",
    "])\n",
    "\n",
    "print(\"Inter-Topic Similarity Metrics:\")\n",
    "display(similarity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Similarity Heatmap\n",
    "# Show heatmap for first version with valid similarity matrix\n",
    "for version_id in version_ids:\n",
    "    results = similarity_results[version_id]\n",
    "    version_name = versions_df[versions_df['id']==version_id]['name'].values[0]\n",
    "    \n",
    "    # Skip versions with fewer than 2 topics (empty similarity matrix)\n",
    "    if results['similarity_matrix'].size == 0:\n",
    "        print(f\"Skipping {version_name}: fewer than 2 topics, no similarity matrix available\")\n",
    "        continue\n",
    "    \n",
    "    fig = px.imshow(\n",
    "        results['similarity_matrix'],\n",
    "        labels=dict(x=\"Topic\", y=\"Topic\", color=\"Cosine Similarity\"),\n",
    "        x=results['topic_ids'],\n",
    "        y=results['topic_ids'],\n",
    "        color_continuous_scale='RdYlGn_r',\n",
    "        title=f\"Inter-Topic Similarity Matrix - {version_name}\"\n",
    "    )\n",
    "    fig.update_layout(height=600, width=700)\n",
    "    fig.show()\n",
    "    break  # Only show first valid heatmap as example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Assignment Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Compute Assignment Quality\n",
    "def compute_assignment_quality(assignments_df, topics_df):\n",
    "    \"\"\"Analyze document-topic assignment confidence and coverage\"\"\"\n",
    "    # Confidence score statistics\n",
    "    confidence_scores = assignments_df['topic_confidence'].dropna()\n",
    "    \n",
    "    # Coverage metrics\n",
    "    total_articles = len(assignments_df)\n",
    "    outliers = len(assignments_df[assignments_df['primary_topic_id'] == -1])\n",
    "    outlier_ratio = outliers / total_articles if total_articles > 0 else 0\n",
    "    \n",
    "    # High-confidence assignment percentages\n",
    "    high_conf_30 = len(confidence_scores[confidence_scores > 0.3]) / len(confidence_scores) if len(confidence_scores) > 0 else 0\n",
    "    high_conf_50 = len(confidence_scores[confidence_scores > 0.5]) / len(confidence_scores) if len(confidence_scores) > 0 else 0\n",
    "    high_conf_70 = len(confidence_scores[confidence_scores > 0.7]) / len(confidence_scores) if len(confidence_scores) > 0 else 0\n",
    "    \n",
    "    # Topic size distribution\n",
    "    topic_sizes = assignments_df[\n",
    "        assignments_df['primary_topic_id'] != -1\n",
    "    ].groupby('primary_topic_id').size()\n",
    "    \n",
    "    return {\n",
    "        'mean_confidence': confidence_scores.mean(),\n",
    "        'median_confidence': confidence_scores.median(),\n",
    "        'std_confidence': confidence_scores.std(),\n",
    "        'outlier_ratio': outlier_ratio,\n",
    "        'coverage': 1 - outlier_ratio,\n",
    "        'high_conf_30': high_conf_30,\n",
    "        'high_conf_50': high_conf_50,\n",
    "        'high_conf_70': high_conf_70,\n",
    "        'num_topics': len(topics_df),\n",
    "        'mean_topic_size': topic_sizes.mean(),\n",
    "        'std_topic_size': topic_sizes.std(),\n",
    "        'min_topic_size': topic_sizes.min(),\n",
    "        'max_topic_size': topic_sizes.max(),\n",
    "        'topic_size_cv': topic_sizes.std() / topic_sizes.mean() if topic_sizes.mean() > 0 else 0\n",
    "    }\n",
    "\n",
    "# Compute for all versions\n",
    "assignment_results = {}\n",
    "for version_id in version_ids:\n",
    "    data = version_data[version_id]\n",
    "    assignment_results[version_id] = compute_assignment_quality(\n",
    "        data['assignments'], data['topics']\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Assignment quality computation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Assignment Quality DataFrame\n",
    "assignment_df = pd.DataFrame([\n",
    "    {\n",
    "        'version_id': vid,\n",
    "        'version_name': versions_df[versions_df['id']==vid]['name'].values[0],\n",
    "        **results\n",
    "    }\n",
    "    for vid, results in assignment_results.items()\n",
    "])\n",
    "\n",
    "print(\"Assignment Quality Metrics:\")\n",
    "display(assignment_df[['version_name', 'mean_confidence', 'coverage', 'outlier_ratio', 'high_conf_50', 'num_topics']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Confidence Distribution Visualization\n",
    "# Histogram of confidence scores for each version\n",
    "for version_id in version_ids:\n",
    "    data = version_data[version_id]\n",
    "    version_name = versions_df[versions_df['id']==version_id]['name'].values[0]\n",
    "    \n",
    "    confidence_scores = data['assignments']['topic_confidence'].dropna()\n",
    "    \n",
    "    fig = px.histogram(\n",
    "        confidence_scores,\n",
    "        nbins=50,\n",
    "        title=f'Confidence Score Distribution - {version_name}',\n",
    "        labels={'value': 'Confidence Score', 'count': 'Frequency'}\n",
    "    )\n",
    "    fig.add_vline(x=0.3, line_dash=\"dash\", line_color=\"red\", annotation_text=\"0.3\")\n",
    "    fig.add_vline(x=0.5, line_dash=\"dash\", line_color=\"orange\", annotation_text=\"0.5\")\n",
    "    fig.add_vline(x=0.7, line_dash=\"dash\", line_color=\"green\", annotation_text=\"0.7\")\n",
    "    fig.update_layout(height=400)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Coverage Comparison\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Coverage (% non-outliers)',\n",
    "    x=assignment_df['version_name'],\n",
    "    y=assignment_df['coverage'] * 100,\n",
    "    marker_color='lightseagreen'\n",
    "))\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Outlier Ratio',\n",
    "    x=assignment_df['version_name'],\n",
    "    y=assignment_df['outlier_ratio'] * 100,\n",
    "    marker_color='indianred'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Topic Coverage Across Versions',\n",
    "    xaxis_title='Version',\n",
    "    yaxis_title='Percentage (%)',\n",
    "    barmode='group',\n",
    "    height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 6: Additional Advanced Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Silhouette Score\n",
    "def compute_silhouette_score_for_version(embeddings_df, assignments_df):\n",
    "    \"\"\"Compute silhouette score for topic clustering quality\"\"\"\n",
    "    # Merge embeddings with topic assignments\n",
    "    merged = embeddings_df.merge(\n",
    "        assignments_df[['article_id', 'primary_topic_id']],\n",
    "        on='article_id'\n",
    "    )\n",
    "    \n",
    "    # Exclude outliers (topic -1)\n",
    "    merged = merged[merged['primary_topic_id'] != -1]\n",
    "    \n",
    "    if len(merged) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Convert embeddings to numpy array\n",
    "    X = np.array(merged['embedding'].tolist())\n",
    "    labels = merged['primary_topic_id'].values\n",
    "    \n",
    "    # Check if we have enough samples\n",
    "    if len(np.unique(labels)) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Compute silhouette score (sample for efficiency)\n",
    "    sample_size = min(10000, len(X))\n",
    "    score = silhouette_score(X, labels, metric='cosine', sample_size=sample_size)\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Compute for all versions\n",
    "silhouette_scores = {}\n",
    "for version_id in version_ids:\n",
    "    print(f\"Computing silhouette score for {version_id}...\")\n",
    "    data = version_data[version_id]\n",
    "    silhouette_scores[version_id] = compute_silhouette_score_for_version(\n",
    "        data['embeddings'], data['assignments']\n",
    "    )\n",
    "    print(f\"  Score: {silhouette_scores[version_id]:.3f}\")\n",
    "\n",
    "silhouette_df = pd.DataFrame([\n",
    "    {\n",
    "        'version_id': vid,\n",
    "        'version_name': versions_df[versions_df['id']==vid]['name'].values[0],\n",
    "        'silhouette_score': score\n",
    "    }\n",
    "    for vid, score in silhouette_scores.items()\n",
    "])\n",
    "\n",
    "print(\"\\nSilhouette Scores:\")\n",
    "display(silhouette_df.sort_values('silhouette_score', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: Composite Quality Score\n",
    "def compute_composite_score(version_id):\n",
    "    \"\"\"Compute weighted composite quality score\"\"\"\n",
    "    # Normalize metrics to 0-1 range\n",
    "    coherence = coherence_results[version_id]['c_v']  # Already 0-1\n",
    "    diversity = diversity_results[version_id]['unique_words_ratio']  # Already 0-1\n",
    "    coverage = assignment_results[version_id]['coverage']  # Already 0-1\n",
    "    silhouette = (silhouette_scores[version_id] + 1) / 2  # Scale from [-1,1] to [0,1]\n",
    "    \n",
    "    # Inverse of overlap (lower overlap is better)\n",
    "    low_overlap = 1 - diversity_results[version_id]['avg_jaccard_overlap']\n",
    "    \n",
    "    # Weighted combination (adjust weights as needed)\n",
    "    composite = (\n",
    "        0.3 * coherence +      # Semantic quality\n",
    "        0.2 * diversity +      # Topic distinctiveness\n",
    "        0.2 * coverage +       # Document coverage\n",
    "        0.2 * silhouette +     # Clustering quality\n",
    "        0.1 * low_overlap      # Topic separation\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'coherence': coherence,\n",
    "        'diversity': diversity,\n",
    "        'coverage': coverage,\n",
    "        'silhouette': silhouette,\n",
    "        'low_overlap': low_overlap,\n",
    "        'composite_score': composite\n",
    "    }\n",
    "\n",
    "composite_results = {}\n",
    "for version_id in version_ids:\n",
    "    composite_results[version_id] = compute_composite_score(version_id)\n",
    "\n",
    "composite_df = pd.DataFrame([\n",
    "    {\n",
    "        'version_id': vid,\n",
    "        'version_name': versions_df[versions_df['id']==vid]['name'].values[0],\n",
    "        **results\n",
    "    }\n",
    "    for vid, results in composite_results.items()\n",
    "])\n",
    "\n",
    "print(\"Composite Quality Scores:\")\n",
    "display(composite_df[['version_name', 'coherence', 'diversity', 'coverage', 'silhouette', 'low_overlap', 'composite_score']].sort_values('composite_score', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: Per-Topic Coherence Analysis\n",
    "# Extract per-topic coherence scores for each version\n",
    "per_topic_coherence = {}\n",
    "\n",
    "for version_id in version_ids:\n",
    "    version_name = versions_df[versions_df['id']==version_id]['name'].values[0]\n",
    "    topics = version_data[version_id]['topics']\n",
    "    \n",
    "    # Get per-topic scores\n",
    "    c_v_scores = coherence_results[version_id]['c_v_per_topic']\n",
    "    c_npmi_scores = coherence_results[version_id]['c_npmi_per_topic']\n",
    "    u_mass_scores = coherence_results[version_id]['u_mass_per_topic']\n",
    "    \n",
    "    # Create DataFrame\n",
    "    topic_scores_df = pd.DataFrame({\n",
    "        'version_id': version_id,\n",
    "        'version_name': version_name,\n",
    "        'topic_id': topics['topic_id'].values,\n",
    "        'topic_name': topics['name'].values,\n",
    "        'article_count': topics['article_count'].values,\n",
    "        'c_v': c_v_scores[:len(topics)],\n",
    "        'c_npmi': c_npmi_scores[:len(topics)],\n",
    "        'u_mass': u_mass_scores[:len(topics)]\n",
    "    })\n",
    "    \n",
    "    per_topic_coherence[version_id] = topic_scores_df\n",
    "\n",
    "# Combine all versions\n",
    "all_topics_df = pd.concat(per_topic_coherence.values(), ignore_index=True)\n",
    "\n",
    "print(f\"Per-topic analysis for {len(all_topics_df)} topics across {len(version_ids)} versions\")\n",
    "print(\"\\nSample (showing first 20 topics):\")\n",
    "display(all_topics_df[['version_name', 'topic_name', 'article_count', 'c_v', 'c_npmi', 'u_mass']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22: Top & Bottom Performing Topics\n",
    "# For each version, identify best and worst topics\n",
    "for version_id in version_ids:\n",
    "    version_name = versions_df[versions_df['id']==version_id]['name'].values[0]\n",
    "    topic_scores = per_topic_coherence[version_id]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Version: {version_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Top 5 topics by C_v\n",
    "    top_5 = topic_scores.nlargest(5, 'c_v')\n",
    "    print(\"\\nüèÜ Top 5 Topics by Coherence (C_v):\")\n",
    "    for idx, row in top_5.iterrows():\n",
    "        print(f\"  {row['topic_name'][:50]:50s} | C_v: {row['c_v']:.3f} | Articles: {int(row['article_count'])}\")\n",
    "    \n",
    "    # Bottom 5 topics by C_v\n",
    "    bottom_5 = topic_scores.nsmallest(5, 'c_v')\n",
    "    print(\"\\n‚ö†Ô∏è  Bottom 5 Topics by Coherence (C_v):\")\n",
    "    for idx, row in bottom_5.iterrows():\n",
    "        print(f\"  {row['topic_name'][:50]:50s} | C_v: {row['c_v']:.3f} | Articles: {int(row['article_count'])}\")\n",
    "    \n",
    "    # Topics with low coherence AND high article count (problematic)\n",
    "    problematic = topic_scores[\n",
    "        (topic_scores['c_v'] < topic_scores['c_v'].median()) &\n",
    "        (topic_scores['article_count'] > topic_scores['article_count'].median())\n",
    "    ]\n",
    "    \n",
    "    if len(problematic) > 0:\n",
    "        print(\"\\nüö® Problematic Topics (Low coherence, High article count):\")\n",
    "        for idx, row in problematic.iterrows():\n",
    "            print(f\"  {row['topic_name'][:50]:50s} | C_v: {row['c_v']:.3f} | Articles: {int(row['article_count'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 23: Per-Topic Coherence Distribution\n",
    "# Violin plot showing coherence distribution per version\n",
    "fig = go.Figure()\n",
    "\n",
    "for version_id in version_ids:\n",
    "    version_name = versions_df[versions_df['id']==version_id]['name'].values[0]\n",
    "    topic_scores = per_topic_coherence[version_id]\n",
    "    \n",
    "    fig.add_trace(go.Violin(\n",
    "        y=topic_scores['c_v'],\n",
    "        name=version_name,\n",
    "        box_visible=True,\n",
    "        meanline_visible=True\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Per-Topic Coherence Distribution Across Versions',\n",
    "    yaxis_title='C_v Coherence Score',\n",
    "    xaxis_title='Version',\n",
    "    showlegend=True,\n",
    "    height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 24: Topic Quality vs Size Scatter Plot\n",
    "# Scatter plot: Article count vs Coherence for all topics\n",
    "for version_id in version_ids:\n",
    "    version_name = versions_df[versions_df['id']==version_id]['name'].values[0]\n",
    "    topic_scores = per_topic_coherence[version_id]\n",
    "    \n",
    "    fig = px.scatter(\n",
    "        topic_scores,\n",
    "        x='article_count',\n",
    "        y='c_v',\n",
    "        hover_data=['topic_name'],\n",
    "        title=f'Topic Quality vs Size - {version_name}',\n",
    "        labels={'article_count': 'Number of Articles', 'c_v': 'Coherence (C_v)'},\n",
    "        size='article_count',\n",
    "        size_max=20\n",
    "    )\n",
    "    \n",
    "    # Add reference lines\n",
    "    fig.add_hline(y=topic_scores['c_v'].mean(), line_dash=\"dash\", line_color=\"red\",\n",
    "                  annotation_text=\"Mean Coherence\")\n",
    "    fig.add_vline(x=topic_scores['article_count'].median(), line_dash=\"dash\", line_color=\"blue\",\n",
    "                  annotation_text=\"Median Size\")\n",
    "    \n",
    "    fig.update_layout(height=500)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 25: Consolidated Metrics Table\n",
    "# Merge all metrics into single DataFrame\n",
    "consolidated_df = (\n",
    "    coherence_df\n",
    "    .merge(diversity_df, on=['version_id', 'version_name'], suffixes=('', '_diversity'))\n",
    "    .merge(similarity_df, on=['version_id', 'version_name'])\n",
    "    .merge(assignment_df, on=['version_id', 'version_name'])\n",
    "    .merge(silhouette_df, on=['version_id', 'version_name'])\n",
    "    .merge(composite_df[['version_id', 'composite_score']], on='version_id')\n",
    ")\n",
    "\n",
    "# Select key columns for display\n",
    "key_columns = [\n",
    "    'version_name', 'c_v', 'unique_words_ratio', 'mean_similarity',\n",
    "    'coverage', 'mean_confidence', 'silhouette_score', 'composite_score'\n",
    "]\n",
    "\n",
    "print(\"Consolidated Metrics (sorted by composite score):\")\n",
    "display(consolidated_df[key_columns].sort_values('composite_score', ascending=False))\n",
    "\n",
    "# Save to CSV\n",
    "consolidated_df.to_csv('topic_evaluation_results.csv', index=False)\n",
    "print(\"\\nResults saved to topic_evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 26: Radar Chart Comparison\n",
    "# Create radar chart comparing versions across normalized metrics\n",
    "from math import pi\n",
    "\n",
    "def create_radar_chart(df, version_name):\n",
    "    \"\"\"Create radar chart for a single version\"\"\"\n",
    "    categories = ['Coherence', 'Diversity', 'Coverage', 'Silhouette', 'Low Overlap']\n",
    "    \n",
    "    # Get values for this version\n",
    "    row = df[df['version_name'] == version_name].iloc[0]\n",
    "    values = [\n",
    "        row['c_v'],\n",
    "        row['unique_words_ratio'],\n",
    "        row['coverage'],\n",
    "        (row['silhouette_score'] + 1) / 2,  # Normalize to 0-1\n",
    "        1 - row['avg_jaccard_overlap']  # Inverse overlap\n",
    "    ]\n",
    "    \n",
    "    # Close the plot\n",
    "    values += values[:1]\n",
    "    \n",
    "    return categories, values\n",
    "\n",
    "# Create radar chart for each version\n",
    "fig = go.Figure()\n",
    "\n",
    "for version_id in version_ids:\n",
    "    version_name = versions_df[versions_df['id']==version_id]['name'].values[0]\n",
    "    categories, values = create_radar_chart(consolidated_df, version_name)\n",
    "    \n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=values,\n",
    "        theta=categories + [categories[0]],\n",
    "        fill='toself',\n",
    "        name=version_name\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(radialaxis=dict(visible=True, range=[0, 1])),\n",
    "    title='Topic Model Quality Comparison (Normalized Metrics)',\n",
    "    showlegend=True,\n",
    "    height=600\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 29: Recommendations & Summary\n",
    "# Generate recommendations based on metrics\n",
    "best_version = consolidated_df.loc[consolidated_df['composite_score'].idxmax()]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TOPIC MODELING EVALUATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nBest Overall Version: {best_version['version_name']}\")\n",
    "print(f\"Composite Score: {best_version['composite_score']:.3f}\\n\")\n",
    "\n",
    "print(\"Breakdown:\")\n",
    "print(f\"  ‚Ä¢ Coherence (C_v): {best_version['c_v']:.3f}\")\n",
    "print(f\"  ‚Ä¢ Topic Diversity: {best_version['unique_words_ratio']:.3f}\")\n",
    "print(f\"  ‚Ä¢ Coverage: {best_version['coverage']:.1%}\")\n",
    "print(f\"  ‚Ä¢ Silhouette Score: {best_version['silhouette_score']:.3f}\")\n",
    "print(f\"  ‚Ä¢ Mean Confidence: {best_version['mean_confidence']:.3f}\")\n",
    "print(f\"  ‚Ä¢ Number of Topics: {int(best_version['num_topics_x'])}\")\n",
    "print(f\"  ‚Ä¢ Outlier Ratio: {best_version['outlier_ratio']:.1%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Version Rankings by Metric:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "metrics_to_rank = {\n",
    "    'Coherence (C_v)': 'c_v',\n",
    "    'Topic Diversity': 'unique_words_ratio',\n",
    "    'Coverage': 'coverage',\n",
    "    'Silhouette Score': 'silhouette_score',\n",
    "    'Mean Confidence': 'mean_confidence'\n",
    "}\n",
    "\n",
    "for metric_name, metric_col in metrics_to_rank.items():\n",
    "    ranked = consolidated_df.sort_values(metric_col, ascending=False)\n",
    "    top_3 = ranked.head(3)['version_name'].tolist()\n",
    "    print(f\"\\n{metric_name}:\")\n",
    "    for i, name in enumerate(top_3, 1):\n",
    "        print(f\"  {i}. {name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Per-Topic Insights:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Best topic overall across all versions\n",
    "best_topic = all_topics_df.loc[all_topics_df['c_v'].idxmax()]\n",
    "print(f\"\\nüèÜ Best Topic Overall (Coherence):\")\n",
    "print(f\"  Version: {best_topic['version_name']}\")\n",
    "print(f\"  Topic: {best_topic['topic_name']}\")\n",
    "print(f\"  Coherence (C_v): {best_topic['c_v']:.3f}\")\n",
    "print(f\"  Articles: {int(best_topic['article_count'])}\")\n",
    "\n",
    "# Topic with most articles\n",
    "largest_topic = all_topics_df.loc[all_topics_df['article_count'].idxmax()]\n",
    "print(f\"\\nüìä Largest Topic:\")\n",
    "print(f\"  Version: {largest_topic['version_name']}\")\n",
    "print(f\"  Topic: {largest_topic['topic_name']}\")\n",
    "print(f\"  Coherence (C_v): {largest_topic['c_v']:.3f}\")\n",
    "print(f\"  Articles: {int(largest_topic['article_count'])}\")\n",
    "\n",
    "# Average coherence range across versions\n",
    "print(f\"\\nüìà Per-Topic Coherence Ranges:\")\n",
    "for version_id in version_ids:\n",
    "    version_name = versions_df[versions_df['id']==version_id]['name'].values[0]\n",
    "    topic_scores = per_topic_coherence[version_id]\n",
    "    print(f\"  {version_name}: {topic_scores['c_v'].min():.3f} - {topic_scores['c_v'].max():.3f} (range: {topic_scores['c_v'].max() - topic_scores['c_v'].min():.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
